# 🏭 데이터 생성 파이프라인 심층 분석 (Data Pipeline Deep Dive)

이 문서는 `.npy` 파일 로딩부터 GPU 상에서의 최종 믹싱까지, **데이터가 흐르는 모든 과정과 텐서(Tensor)의 차원(Dimension) 변화**를 아주 상세히 설명합니다. 파이썬과 PyTorch가 처음인 분들도 이해할 수 있도록 단계별로 시각화했습니다.

---

## 🏗️ 1. 전체 흐름 요약 (The Big Picture)

우리의 데이터 파이프라인은 속도를 위해 **"가벼운 건 CPU가, 무거운 건 GPU가"** 처리하도록 설계되어 있습니다.

```text
[Disk (.npy)] 
      │
      │ (CPU: 고속 로딩)
      ▼
  [Dataset]
      │
      │ (CPU: 배치 묶기)
      ▼
 [DataLoader]
      │
      │ (GPU 이동)
      ▼
{GPU Synthesis} ─── (FFT Convolution) ───▶ [최종 학습 데이터]
```

---

## 🛠️ 2. 단계별 상세 분석

### Step 1: CPU에서의 데이터 준비 (Dataset Level)
**위치:** `src/data/dataset.py` - `__getitem__`

하드디스크에서 필요한 재료(음성, 노이즈, 공간 정보)를 꺼내오는 단계입니다. 여기서 무거운 연산은 하지 않고 '날것(Raw)' 그대로 가져옵니다.

1.  **클린 스피치 (Clean Speech)**
    *   랜덤으로 사람 목소리 파일 하나를 고릅니다.
    *   3초 길이로 자릅니다. (Sample Rate 16kHz 기준 48,000 샘플)
    *   **형상(Shape):** `(48000,)` (1D 텐서)

2.  **공간 정보 (RIR)**
    *   랜덤으로 방(Room) 하나를 고릅니다.
    *   방의 울림 정보(Impulse Response)를 가져옵니다.
    *   **형상:** `(M, S, L)`
        *   `M (Mics)`: 마이크 개수 (예: 4개)
        *   `S (Sources)`: 소리 발생 위치 개수 (예: 8개)
        *   `L (Length)`: 울림의 길이 (예: 8,000 ~ 16,000)

3.  **노이즈 (Noise)**
    *   스피치 외에 나머지 소스 개수(`S-1`)만큼 노이즈 파일을 랜덤으로 뽑습니다.
    *   **형상:** `(S-1, 48000)` (여러 개의 1D 텐서)

#### 📝 참고: 3초보다 짧은 파일은 어떻게 되나요? (Zero Padding)
데이터셋에 3초(48,000 샘플)보다 짧은 음원이 있어도 버리지 않고 사용합니다. 부족한 뒷부분은 **침묵(0)**으로 채워 넣는데, 이를 **제로 패딩(Zero Padding)**이라고 부릅니다. 반대로 3초보다 긴 파일은 랜덤한 구간을 3초만큼만 잘라서(Crop) 사용합니다.

> **💡 결과물 (하나의 샘플):**
> *   `raw_speech`: `(48000,)`
> *   `raw_noises`: `(7, 48000)` (소스가 8개일 때)
> *   `rir_tensor`: `(4, 8, 16000)`

---

### Step 2: 배칭 (Batching)
**위치:** `src/data/datamodule.py` (PyTorch 내부 동작)

CPU가 준비한 샘플들을 **박스(Batch)** 에 담습니다. 예를 들어 `batch_size=16`이면 16개를 묶습니다.
이 과정에서 차원(Dimension)의 **맨 앞에 `B` (Batch Size)가 추가**됩니다.

> **📦 배칭 후 텐서 형상:**
> *   `raw_speech`: `(16, 48000)`
> *   `raw_noises`: `(16, 7, 48000)`
> *   `rir_tensor`: `(16, 4, 8, 16000)`

---

### Step 3: GPU 합성 (On-the-fly Synthesis) 🔥 핵심
**위치:** `src/modules/se_module.py` - `_apply_gpu_synthesis`

이제 모든 재료가 GPU로 넘어왔습니다. 여기서 **"공간감 입히기(Convolution)"**와 **"섞기(Mixing)"**가 일어납니다.

#### 1. 스피치 공간화 (Convolve Speech with RIR)
클린 스피치에 첫 번째 소스 위치의 RIR을 곱해서(Convolution), 마치 그 방 안에서 말하는 것처럼 만듭니다.
*   입력: `(B, 1, 48000)` (Speech)
*   필터: `(B, M, 16000)` (RIR의 0번 소스)
*   연산: `FFT Convolution` (고속 연산)
*   **결과:** `(B, M, 48000)` -> **마이크 4개에 도달한 목소리**가 생성됨!

#### 2. 노이즈 공간화 (Convolve Noises)
나머지 노이즈들도 각각 다른 위치의 RIR을 곱해서 공간감을 입히고 하나로 합칩니다.
*   입력: `(B, 7, 48000)` (Noises)
*   필터: `(B, M, 16000)` (RIR의 1~7번 소스)
*   **결과:** `(B, M, 48000)` -> **모든 방향에서 들려온 잡음의 합**

#### 3. 골전도 센서 모델링 (BCM - Optional)
마이크 중 하나는 "골전도 마이크"라고 가정하고, 소리를 뭉개뜨립니다(Low-pass Filter).
*   마지막 채널(`M-1`번째)에 500Hz 이하만 통과시키는 필터를 씌웁니다.

#### 4. SNR 조절 (Scaling)
목소리와 잡음의 비율(Signal-to-Noise Ratio)을 맞춥니다.
*   사용자가 설정한 SNR(예: 5dB)에 맞춰 잡음의 볼륨을 키우거나 줄입니다.

#### 5. 최종 믹싱 (Final Mix)
공간화된 목소리와 공간화된 잡음을 더합니다.
*   `Noisy` = `Spatial_Speech` + `Scaled_Spatial_Noise`
*   **최종 형상:** `(B, M, 48000)` (Batch, Mics, Time)

---

## 🎯 3. 최종 출력 데이터 (Model Input)

GPU 합성 과정을 거쳐 모델에 `forward()`로 들어가는 최종 데이터는 다음과 같습니다.

1.  **입력 (Noisy Audio):** `(16, 4, 48000)`
    *   16개의 샘플
    *   4개의 마이크 채널 (3개 Air Mic + 1개 BCM)
    *   3초 길이의 파형

2.  **정답 (Clean Target):** `(16, 4, 48000)`
    *   잡음이 섞이기 전, "공간감만 입혀진 목소리"입니다.
    *   모델은 Noisy를 받아서 이 Clean Target을 맞추도록 학습합니다.

---

## ❓ FAQ: 파이썬/토치 초보자를 위한 팁

**Q. 왜 텐서(Tensor)를 쓰나요?**
A. 텐서는 "숫자로 된 직육면체 큐브"라고 생각하세요. GPU는 이 큐브들을 한꺼번에 계산하는 것을 엄청나게 잘합니다. 16개의 오디오를 for문으로 하나씩 돌리면 느리지만, 텐서로 묶어서 `(16, ...)`으로 만들면 한 방에 처리됩니다.

**Q. `Dimension`이 왜 중요한가요?**
A. 레고 블록을 조립할 때 구멍 개수가 맞아야 끼울 수 있듯이, 텐서끼리 계산(더하기, 곱하기)하려면 차원(모양)이 맞아야 합니다. 에러의 90%는 이 모양이 안 맞아서 생깁니다.

**Q. 48000은 어디서 나온 숫자인가요?**
A. `16000 Hz` (1초에 16,000개 샘플) × `3초` = `48,000`입니다.
